# app/main.py
# Streamlit UI cho Hybrid RAG: Neo4j (NL2Cypher) + FAISS, h·ª£p nh·∫•t theo ID v√† t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi
import os
import json
import traceback
from typing import List, Dict, Any

import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# Local modules
from app.retrievers.hybrid_retriever import HybridRetriever
from app.retrievers.vector_tools import VectorClient, Passage

# C·∫•u h√¨nh
load_dotenv()

def get_var(key, default=None, section="general"):
    try:
        return st.secrets[section].get(key, default)
    except Exception:
        return os.getenv(key, default)
OPENAI_MODEL = get_var("OPENAI_MODEL", "gpt-4o-mini")
ANSWER_RULE_PATH = get_var("ANSWER_RULE_PATH", "app/prompts/answer_synthesis.txt")
OPENAI_API_KEY = get_var("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
client = OpenAI(api_key=OPENAI_API_KEY)


# ===============================
# üîß Helper functions
# ===============================
def load_answer_rule(path: str = ANSWER_RULE_PATH) -> str:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y rule t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return f.read().strip()


def build_id_map_from_graph_records(records: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """T·∫°o map id -> record (thu·ªôc t√≠nh t·ª´ Neo4j)."""
    id_map = {}
    for r in records or []:
        rid = str(r.get("id") or "").strip()
        if rid:
            id_map[rid] = r
    return id_map


def vector_fetch_by_ids(vclient: VectorClient, ids: List[str], limit: int = 3) -> List[Passage]:
    """Truy xu·∫•t l·∫°i c√°c b√†i theo ID t·ª´ VectorDB."""
    vs = vclient._load_vs()
    results: List[Passage] = []
    wanted = set([str(x).strip() for x in ids if x])
    try:
        for _, doc in (vs.docstore._dict or {}).items():
            mid = (doc.metadata or {}).get("id")
            if mid and str(mid).strip() in wanted:
                results.append(
                    Passage(id=str(mid).strip(), text=doc.page_content or "", score=None, metadata=doc.metadata or {})
                )
                if len(results) >= limit:
                    break
    except Exception:
        pass
    return results


def select_top3_by_priority(
    graph_ids: List[str],
    vector_passages: List[Passage],
    vclient: VectorClient,
    graph_id_map: Dict[str, Dict[str, Any]],
    fill_limit: int = 3
) -> List[Passage]:
    """Ch·ªçn top 3 b√†i ∆∞u ti√™n tr√πng ID gi·ªØa Graph v√† Vector."""
    picked: List[Passage] = []
    used_ids = set()
    graph_ids = [str(x).strip() for x in graph_ids if str(x).strip()]
    vector_by_id = {str(p.id).strip(): p for p in vector_passages if p.id}

    # 1Ô∏è‚É£ Overlap gi·ªØa Graph & Vector
    for gid in graph_ids:
        if gid in vector_by_id and gid not in used_ids:
            picked.append(vector_by_id[gid])
            used_ids.add(gid)
            if len(picked) >= fill_limit:
                return picked

    # 2Ô∏è‚É£ Graph c√≥ ID nh∆∞ng Vector ch∆∞a c√≥ ‚Üí c·ªë fetch theo ID
    missing_from_vector = [gid for gid in graph_ids if gid not in used_ids and gid not in vector_by_id]
    if missing_from_vector:
        fetched = vector_fetch_by_ids(vclient, missing_from_vector, limit=(fill_limit - len(picked)))
        for p in fetched:
            if p.id and p.id not in used_ids:
                picked.append(p)
                used_ids.add(p.id)
                if len(picked) >= fill_limit:
                    return picked

    # 3Ô∏è‚É£ B·ªï sung t·ª´ vector_passages c√≤n l·∫°i
    for p in vector_passages:
        pid = str(p.id).strip() if p.id else None
        if pid and pid not in used_ids:
            picked.append(p)
            used_ids.add(pid)
        if len(picked) >= fill_limit:
            break

    return picked[:fill_limit]


def build_synthesis_input(chosen_passages: List[Passage], graph_id_map: Dict[str, Dict[str, Any]]) -> str:
    """T·∫°o text c√≥ c·∫•u tr√∫c ƒë·ªÉ g·ª≠i LLM t·ªïng h·ª£p."""
    blocks = []
    for p in chosen_passages:
        pid = str(p.id).strip() if p.id else None
        graph_info = graph_id_map.get(pid) if pid else None
        block = {
            "id": pid or "N/A",
            "graph": graph_info or {},
            "vector_text": (p.text or "").strip(),
        }
        blocks.append(block)

    pretty = []
    for b in blocks:
        pretty.append(
            f"ID: {b['id']}\nGRAPH: {json.dumps(b['graph'], ensure_ascii=False)}\nTEXT: {b['vector_text']}"
        )
    return "\n\n---\n\n".join(pretty)


def llm_summarize_answer(client: OpenAI, user_query: str, synthesis_rule: str, synthesis_payload: str, model: str) -> str:
    """T·ªïng h·ª£p ƒë·∫ßu ra cu·ªëi c√πng b·∫±ng LLM."""
    prompt = f"""{synthesis_rule}

D·ªØ li·ªáu ƒë·∫ßu v√†o:
{synthesis_payload}

C√¢u h·ªèi ng∆∞·ªùi d√πng:
{user_query}
"""
    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
    )
    return resp.choices[0].message.content.strip()


# ===============================
# üñ•Ô∏è Streamlit UI
# ===============================
def main():
    st.set_page_config(page_title="Hybrid RAG - B·∫•t ƒë·ªông s·∫£n H√† N·ªôi", page_icon="üè†", layout="wide")
    st.title("üè† Hybrid RAG cho B·∫•t ƒë·ªông s·∫£n H√† N·ªôi")
    st.caption("K·∫øt h·ª£p Neo4j (Graph) + FAISS (Vector) ¬∑ Truy v·∫•n 1 l·∫ßn Graph duy nh·∫•t ¬∑ Gi·ªõi h·∫°n 3 cƒÉn / c√¢u tr·∫£ l·ªùi")

    with st.sidebar:
        st.header("‚öôÔ∏è C√†i ƒë·∫∑t")
        model = st.text_input("OPENAI_MODEL", value=OPENAI_MODEL)
        top_k = st.slider("S·ªë k·∫øt qu·∫£ Vector (k)", min_value=5, max_value=20, value=10)
        limit_ids = st.slider("Gi·ªõi h·∫°n ID tr·∫£ l·ªùi", min_value=1, max_value=5, value=3)
        show_debug = st.checkbox("üß© Hi·ªÉn th·ªã debug (IDs & m√¥ t·∫£)", value=True)

    # Nh·∫≠p c√¢u h·ªèi
    user_query = st.text_input("üí¨ Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", placeholder="V√≠ d·ª•: T√¨m nh√† 5 t·∫ßng s·ªï ƒë·ªè ch√≠nh ch·ªß t·∫°i Thanh Xu√¢n")
    run = st.button("üîé T√¨m ki·∫øm")

    if run and user_query.strip():
        try:
            client = OpenAI()
            synth_rule = load_answer_rule()
            hybrid = HybridRetriever()
            vclient = hybrid.vector

            # 1Ô∏è‚É£ Hybrid Search (Graph + Vector, ch·ªâ query Graph 1 l·∫ßn)
            st.info("‚è≥ ƒêang truy v·∫•n d·ªØ li·ªáu t·ª´ Neo4j v√† FAISS...")
            hybrid_result = hybrid.search(user_query=user_query, top_k=top_k)
            graph_records = hybrid_result["graph_records"]
            graph_ids = hybrid_result["graph_ids"]
            vector_passages = hybrid_result["vector_passages"]

            # 2Ô∏è‚É£ K·∫øt h·ª£p d·ªØ li·ªáu
            graph_id_map = build_id_map_from_graph_records(graph_records)
            chosen_passages = select_top3_by_priority(
                graph_ids, vector_passages, vclient, graph_id_map, fill_limit=limit_ids
            )

            # Debug
            if show_debug:
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("üìä IDs t·ª´ Graph")
                    st.write(graph_ids[:20])
                with col2:
                    st.subheader("üìö IDs trong Vector (t·ª´ k k·∫øt qu·∫£)")
                    st.write([p.id for p in vector_passages if p.id][:20])

                st.subheader("‚úÖ ID ƒë∆∞·ª£c ch·ªçn (∆∞u ti√™n tr√πng, t·ªëi ƒëa N)")
                st.write([p.id for p in chosen_passages])

                st.subheader("üìù Snippet m√¥ t·∫£ (Vector)")
                for p in chosen_passages:
                    st.markdown(f"- **ID {p.id or 'N/A'}** ¬∑ _{(p.text or '')[:200]}{'...' if p.text and len(p.text)>200 else ''}_")

            # 3Ô∏è‚É£ Chu·∫©n b·ªã d·ªØ li·ªáu cho LLM
            synthesis_payload = build_synthesis_input(chosen_passages, graph_id_map)

            # 4Ô∏è‚É£ G·ªçi LLM ƒë·ªÉ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi
            st.write("üß† ƒêang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...")
            answer = llm_summarize_answer(client, user_query, synth_rule, synthesis_payload, model)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.markdown("---")
            st.subheader("‚ú® C√¢u tr·∫£ l·ªùi")
            st.write(answer)

            # 5Ô∏è‚É£ B·∫£ng d·ªØ li·ªáu chi ti·∫øt
            with st.expander("üìã Xem d·ªØ li·ªáu ƒë√£ h·ª£p nh·∫•t (debug)"):
                merged_rows = []
                for p in chosen_passages:
                    pid = str(p.id).strip() if p.id else None
                    row = {"id": pid, "text_len": len(p.text or "")}
                    row.update(graph_id_map.get(pid, {}))
                    merged_rows.append(row)
                try:
                    import pandas as pd
                    st.dataframe(pd.DataFrame(merged_rows))
                except Exception:
                    st.json(merged_rows)

        except Exception as e:
            st.error("‚ùå L·ªói khi x·ª≠ l√Ω truy v·∫•n.")
            st.exception(e)
            st.text(traceback.format_exc())

    st.markdown("---")
    st.caption("¬© Hybrid RAG ‚Ä¢ Neo4j + FAISS ‚Ä¢ LangChain-style pipeline")


if __name__ == "__main__":
    main()
